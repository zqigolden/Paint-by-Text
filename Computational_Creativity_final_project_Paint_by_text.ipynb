{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Computational Creativity final project - Paint by text.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMuwH4pbbrCCH1Bdm2cVpg6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zqigolden/Paint-by-Text/blob/master/Computational_Creativity_final_project_Paint_by_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZwraIF6dwQW"
      },
      "outputs": [],
      "source": [
        "#@title Init the enveronment \n",
        "#@markdown It would take several minutes to make everything ready\n",
        "%%capture \n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers.git\n",
        "!curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1'\n",
        "!curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1'\n",
        "!pip install Pillow Ipython ftfy omegaconf pytorch-lightning einops kornia\n",
        "\n",
        "#packages from the system\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "from urllib.request import urlopen\n",
        "\n",
        "#packages from the pip\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "from IPython.display import display, Image as disImg\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch import nn, optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from omegaconf import OmegaConf\n",
        "import kornia.augmentation as K\n",
        "\n",
        "#packages from the git\n",
        "sys.path.insert(1, '/content/taming-transformers')\n",
        "from taming.models import cond_transformer, vqgan\n",
        "from CLIP import clip\n",
        "\n",
        "# support classes\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n",
        "            K.RandomPerspective(0.7,p=0.7),\n",
        "            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n",
        "            K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7),\n",
        "            \n",
        ")\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        \n",
        "        for _ in range(self.cutn):\n",
        "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
        "            cutouts.append(cutout)\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf'), gray=False):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "        self.gray=gray\n",
        "\n",
        "    def forward(self, input, gray_input):\n",
        "        if self.gray:\n",
        "            input = gray_input\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Obtain and cut out the image { display-mode: \"form\", run: \"auto\"}\n",
        "icon_size = 600 #@param {type:\"integer\"}\n",
        "seed = 42\n",
        "bg_noise_opacity = 0.2 #@param {type:\"number\"}\n",
        "icon_opacity = 0.8 #@param {type:\"number\"}\n",
        "is_gray = True #@param {type:\"boolean\"}\n",
        "choosed_img = 'twitter_logo' #@param [\"twitter_logo\", \"dog\", \"bird\", \"lena\", \"custom_url\"] {type:\"string\"}\n",
        "\n",
        "custom_url = '' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown There are some image examples for the creatavity initialize. Also you can try your own images by replace the url variable.\n",
        "\n",
        "urls = {}\n",
        "urls['bird'] = r'https://upload.wikimedia.org/wikipedia/commons/9/9a/Gull_portrait_ca_usa.jpg'\n",
        "urls['lena'] = r'https://p0.itc.cn/q_70/images03/20210511/ac77f8122cc240979568a365faae4dc3.jpeg'\n",
        "urls['dog'] = r'https://img.freepik.com/free-photo/french-bulldog-young-dog-posing_155003-34898.jpg?t=st=1649881371~exp=1649881971~hmac=7216f9bbd3e18036eba085d730a88738b59c3faf6ddd98f62c268a06e63e85b0&w=2000'\n",
        "urls['twitter_logo'] = r'https://about.twitter.com/content/dam/about-twitter/en/brand-toolkit/brand-download-img-1.jpg.twimg.1920.jpg'\n",
        "urls['custom_url'] = custom_url\n",
        "url = urls[choosed_img]\n",
        "img = Image.open(urlopen(url))\n",
        "\n",
        "icon_img = img.convert(\"RGBA\")\n",
        "\n",
        "\n",
        "w, h = icon_img.size\n",
        "\n",
        "if w > h:\n",
        "    left, upper, right, lower = ((w - h) // 2, 0, w - (w - h) // 2, h)\n",
        "    icon_img = icon_img.crop((left, upper, right, lower))\n",
        "else:\n",
        "    left, upper, right, lower = (0, (h - w) // 2, w, h - (h - w) // 2)\n",
        "\n",
        "icon_img = icon_img.resize((icon_size, icon_size), Image.ANTIALIAS)\n",
        "\n",
        "sketch_array = np.asarray(icon_img.convert(\"L\")).T\n",
        "\n",
        "# https://stackoverflow.com/a/765829\n",
        "pixdata = icon_img.load()\n",
        "width, height = icon_img.size\n",
        "for y in range(height):\n",
        "    for x in range(width):\n",
        "        if sketch_array[x, y] >= 250:\n",
        "            pixdata[x, y] = (255, 255, 255, 0)\n",
        "\n",
        "if icon_opacity < 1:\n",
        "    icon_img = Image.blend(\n",
        "            Image.new(\"RGBA\", (icon_size, icon_size), (0, 0, 0, 0)),\n",
        "            icon_img,\n",
        "            icon_opacity,\n",
        "        )\n",
        "icon_bg = Image.new(\"RGBA\", (icon_size, icon_size), 'white')\n",
        "if seed:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "noise = np.uint8(np.random.rand(icon_size, icon_size) * 255)\n",
        "noise_array = np.stack(\n",
        "    [\n",
        "        noise,\n",
        "        noise,\n",
        "        noise,\n",
        "        np.uint8(np.full((icon_size, icon_size), 255 * bg_noise_opacity)),\n",
        "    ],\n",
        "    axis=2,\n",
        ")\n",
        "noise_img = Image.fromarray(\n",
        "    noise_array,\n",
        "    mode=\"RGBA\",\n",
        ")\n",
        "\n",
        "\n",
        "icon_bg = Image.alpha_composite(icon_bg, noise_img)\n",
        "icon_bg.paste(icon_img, (0, 0), icon_img)\n",
        "if is_gray:\n",
        "    icon_bg = ImageOps.grayscale(icon_bg)\n",
        "icon_bg.save(\"content_with_noise.png\")\n",
        "display(icon_bg)"
      ],
      "metadata": {
        "id": "XKEzADyae9TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training parameters setting { display-mode: \"form\", run: \"auto\"}\n",
        "prompt_image_path = \"content_with_noise.png\"\n",
        "seed = 42\n",
        "init_image = \"\"\n",
        "target_images = \"\"\n",
        "#@markdown Input the prompt texts, multiple tests can be connected with \" | \" and set weight by \":number\"\n",
        "#@markdown Example: `A white bird flies in the blue sky:2 | Vincent Van Gogh style`\n",
        "texts = 'twitter logo | rainbow background' #@param {type:\"string\"}\n",
        "width = 600\n",
        "height = 600\n",
        "use_init_image = True #@param {type:\"boolean\"}\n",
        "if use_init_image:\n",
        "  assert os.path.exists(prompt_image_path), f\"Image {prompt_image_path} not found\"\n",
        "  init_image = prompt_image_path\n",
        "\n",
        "use_target_image = True #@param {type:\"boolean\"}\n",
        "if use_target_image:\n",
        "  assert os.path.exists(prompt_image_path), f\"Image {prompt_image_path} not found\"\n",
        "  target_images = prompt_image_path\n",
        "enable_postprocessing = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "learning_rate = 0.2 #@param {type:\"slider\", min:0.00, max:0.30, step:0.01}\n",
        "max_steps = 500 #@param {type:\"integer\"}\n",
        "images_interval = 50 #@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "ysxHwAHmfRZj"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start AI Image Generation!\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "\n",
        "!rm -rf steps\n",
        "!mkdir steps\n",
        "model_name = 'vqgan_imagenet_f16_16384'\n",
        "\n",
        "if seed == -1:\n",
        "    seed = None\n",
        "if init_image == \"None\":\n",
        "    init_image = None\n",
        "if target_images == \"None\" or not target_images:\n",
        "    model_target_images = []\n",
        "else:\n",
        "    model_target_images = target_images.split(\"|\")\n",
        "    model_target_images = [image.strip() for image in model_target_images]\n",
        "\n",
        "model_texts = [phrase.strip() for phrase in texts.split(\"|\")]\n",
        "if model_texts == ['']:\n",
        "    model_texts = []\n",
        "\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    prompts=model_texts,\n",
        "    image_prompts=model_target_images,\n",
        "    noise_prompt_seeds=[],\n",
        "    noise_prompt_weights=[],\n",
        "    size=[width, height],\n",
        "    init_image=init_image,\n",
        "    init_weight=0.,\n",
        "    clip_model='ViT-B/32',\n",
        "    vqgan_config=f'{model_name}.yaml',\n",
        "    vqgan_checkpoint=f'{model_name}.ckpt',\n",
        "    step_size=learning_rate,\n",
        "    cutn=32,\n",
        "    cut_pow=1.,\n",
        "    display_freq=images_interval,\n",
        "    seed=seed,\n",
        ")\n",
        "from urllib.request import urlopen\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "if model_texts:\n",
        "    print('Using texts:', model_texts)\n",
        "if model_target_images:\n",
        "    print('Using image prompts:', model_target_images)\n",
        "if args.seed is None:\n",
        "    seed = torch.seed()\n",
        "else:\n",
        "    seed = args.seed\n",
        "torch.manual_seed(seed)\n",
        "print('Using seed:', seed)\n",
        "\n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "# clock=deepcopy(perceptor.visual.positional_embedding.data)\n",
        "# perceptor.visual.positional_embedding.data = clock/clock.max()\n",
        "# perceptor.visual.positional_embedding.data=clamp_with_grad(clock,0,1)\n",
        "\n",
        "cut_size = perceptor.visual.input_resolution\n",
        "\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "sideX, sideY = toksX * f, toksY * f\n",
        "\n",
        "if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "    e_dim = 256\n",
        "    n_toks = model.quantize.n_embed\n",
        "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "else:\n",
        "    e_dim = model.quantize.e_dim\n",
        "    n_toks = model.quantize.n_e\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "# z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "# z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "# normalize_imagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                            std=[0.229, 0.224, 0.225])\n",
        "\n",
        "if args.init_image:\n",
        "    if 'http' in args.init_image:\n",
        "        img = Image.open(urlopen(args.init_image))\n",
        "    else:\n",
        "        img = Image.open(args.init_image)\n",
        "    pil_image = img.convert('RGB')\n",
        "    if pil_image.size != (width, height):\n",
        "      print(f\"Resizing source image to {width}x{height}\")\n",
        "      pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "    pil_tensor = TF.to_tensor(pil_image)\n",
        "    z, *_ = model.encode(pil_tensor.to(device).unsqueeze(0) * 2 - 1)\n",
        "else:\n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "    # z = one_hot @ model.quantize.embedding.weight\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        z = one_hot @ model.quantize.embed.weight\n",
        "    else:\n",
        "        z = one_hot @ model.quantize.embedding.weight\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n",
        "    z = torch.rand_like(z)*2\n",
        "z_orig = z.clone()\n",
        "z.requires_grad_(True)\n",
        "opt = optim.Adam([z], lr=args.step_size)\n",
        "scheduler = StepLR(opt, step_size=5, gamma=0.95)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "\n",
        "\n",
        "pMs = []\n",
        "losses = []\n",
        "\n",
        "# text prompts encoded by clip\n",
        "for prompt in args.prompts:\n",
        "    txt, weight, stop = parse_prompt(prompt)\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "# target image prompts encoded by clip\n",
        "for prompt in args.image_prompts:\n",
        "    path, weight, stop = parse_prompt(prompt)\n",
        "    img = Image.open(path)\n",
        "    pil_image = img.convert('RGB')\n",
        "    img = resize_image(pil_image, (sideX, sideY))\n",
        "    batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "    embed = perceptor.encode_image(normalize(batch)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop, gray=enable_postprocessing).to(device))\n",
        "\n",
        "for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "    pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "def synth(z):\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
        "    else:\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def checkin(i, losses):\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "    out = synth(z)\n",
        "    TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "    display(disImg('progress.png'))\n",
        "\n",
        "def ascend_txt():\n",
        "    # global i\n",
        "    out = synth(z)\n",
        "    out_gray = TF.rgb_to_grayscale(out, num_output_channels=3)\n",
        "    feature = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "    feature_gray = perceptor.encode_image(normalize(make_cutouts(out_gray))).float()\n",
        "    result = []\n",
        "\n",
        "    if args.init_weight:\n",
        "        # result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "        result.append(F.mse_loss(z, torch.zeros_like(z_orig)) * ((1/torch.tensor(i*2 + 1))*args.init_weight) / 2)\n",
        "    for prompt in pMs:\n",
        "        result.append(prompt(feature, feature_gray))\n",
        "    img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = Image.fromarray(img)\n",
        "    # imageio.imwrite(f'./steps/{i:03d}.png', np.array(img))\n",
        "\n",
        "    img.save(f\"./steps/{i:03d}.png\")\n",
        "    return result\n",
        "\n",
        "def train(i):\n",
        "    opt.zero_grad()\n",
        "    lossAll = ascend_txt()\n",
        "    if i % args.display_freq == 0:\n",
        "        checkin(i, lossAll)\n",
        "    losses.append([i.item() for i in lossAll])\n",
        "    loss = sum(lossAll)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    scheduler.step()\n",
        "    with torch.no_grad():\n",
        "        z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "losses = []\n",
        "try:\n",
        "    for i in tqdm(range(max_steps)):\n",
        "        train(i)\n",
        "    checkin(max_steps, ascend_txt())\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "__bNyl7RqA8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Draw the losses\n",
        "import matplotlib.pyplot as plt\n",
        "len_sum = len(losses[0])\n",
        "text_loss = [i[0] for i in losses]\n",
        "plt.plot(text_loss, label = 'text loss')\n",
        "if len_sum >= 2:\n",
        "    image_loss = [i[-1] for i in losses]\n",
        "    plt.plot(image_loss, label = 'target image loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('train_record.png', dpi=300)"
      ],
      "metadata": {
        "id": "6bl6ar1QxWFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T4KNZkpNcQwL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}